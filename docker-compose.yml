name: api-template

networks:
  api-template-network:
    name: api-template-network
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.10.0/24

x-deploy: &gpu-deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]

services:
  api-template-app:
    # image: ghcr.io/ilkersigirci/api-template-app:latest
    image: api-template-app:latest
    container_name: api-template-app
    build:
      context: .
      dockerfile: docker/Dockerfile
      # target: development
      target: production
    networks:
      - api-template-network
    ports:
      - 8000:8000
    restart: "no"
    security_opt:
      - no-new-privileges:true
    tty: true
    command: ["python", "-m", "app.__main__"]
    env_file:
      - .env
    environment:
      - PORT=8000
      - RELOAD=False
    # For self-hosted Hatchet, use:
    # depends_on:
    #   hatchet-lite:
    #     condition: service_started

  api-workers-general:
    # image: ghcr.io/ilkersigirci/api-workers-general:latest
    image: api-workers-general:latest
    container_name: api-workers-general
    build:
        context: api-workers-general
        dockerfile: Dockerfile
        # target: development
        target: production
        additional_contexts:
          api-shared: api-shared
    networks:
        - api-template-network
    restart: "no"
    security_opt:
      - no-new-privileges:true
    tty: true
    command: ["python", "-m", "worker.main"]
    env_file:
      - .env
    environment:
      - HATCHET_WORKER_NAME=general-worker
      - HATCHET_WORKER_SLOTS=${WORKER_COUNT:-5}
    # For self-hosted Hatchet, use:
    # depends_on:
    #   hatchet-lite:
    #     condition: service_started

  api-workers-ml:
    # image: ghcr.io/ilkersigirci/api-workers-ml:latest
    image: api-workers-ml:latest
    container_name: api-workers-ml
    build:
        context: api-workers-ml
        dockerfile: Dockerfile
        # target: development
        target: production
        additional_contexts:
          api-shared: api-shared
    networks:
        - api-template-network
    restart: "no"
    security_opt:
      - no-new-privileges:true
    <<: *gpu-deploy
    tty: true
    command: ["python", "-m", "worker.main"]
    env_file:
      - .env
    environment:
      - HATCHET_WORKER_NAME=ml-worker
      - HATCHET_WORKER_SLOTS=${ML_WORKER_COUNT:-3}
    # For self-hosted Hatchet, use:
    # depends_on:
    #   hatchet-lite:
    #     condition: service_started
